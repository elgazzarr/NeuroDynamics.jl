{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Latent SDE with differentiable drift and diffusion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NeuroDynamics, Lux, Random, Plots, DifferentialEquations, ComponentArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create an example forced latent SDE with differentiable drift and diffusion functions.\n",
    "\n",
    "For the encoder, we will use a `Recurrent_Encoder` which will take the input sequence and return the hidden state of the RNN at the last time step. This hidden state will be used as the initial condition for the SDE solver. It will also return a context vector which will be used to condition augmented SDE. \n",
    "\n",
    "The generative SDE will be defined with a `ModernWilsonCowan` drift and a 1 layer network for the diffusion.\n",
    "The augmented SDE will have an MLP for the drift and share the same diffusion with the generative SDE. \n",
    "\n",
    "The decoder is an MLP with `Poisson` noise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentUDE(\n",
       "    obs_encoder = Encoder(\n",
       "        linear_net = Dense(100 => 64),  \u001b[90m# 6_464 parameters\u001b[39m\n",
       "        init_net = Chain(\n",
       "            layer_1 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#34#37\"{Int64}(50)),\n",
       "            layer_2 = Recurrence(\n",
       "                cell = LSTMCell(64 => 64),  \u001b[90m# 33_024 parameters\u001b[39m\u001b[90m, plus 1\u001b[39m\n",
       "            ),\n",
       "            layer_3 = BranchLayer(\n",
       "                layer_1 = Dense(64 => 10),  \u001b[90m# 650 parameters\u001b[39m\n",
       "                layer_2 = Dense(64 => 10, softplus),  \u001b[90m# 650 parameters\u001b[39m\n",
       "            ),\n",
       "        ),\n",
       "        context_net = Chain(\n",
       "            layer_1 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#35#38\"()),\n",
       "            layer_2 = Recurrence(\n",
       "                cell = LSTMCell(64 => 32),  \u001b[90m# 12_416 parameters\u001b[39m\u001b[90m, plus 1\u001b[39m\n",
       "            ),\n",
       "            layer_3 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#36#39\"()),\n",
       "        ),\n",
       "    ),\n",
       "    ctrl_encoder = NoOpLayer(),\n",
       "    dynamics = SDE(\n",
       "        drift = ModernWilsonCowan(10, 10, WeightInitializers.ones32, WeightInitializers.glorot_uniform, WeightInitializers.glorot_uniform, WeightInitializers.ones32),  \u001b[90m# 220 parameters\u001b[39m\n",
       "        drift_aug = Chain(\n",
       "            layer_1 = Dense(42 => 64, softplus),  \u001b[90m# 2_752 parameters\u001b[39m\n",
       "            layer_2 = Dense(64 => 10, tanh_fast),  \u001b[90m# 650 parameters\u001b[39m\n",
       "        ),\n",
       "        diffusion = Dense(10 => 10, sigmoid_fast),  \u001b[90m# 110 parameters\u001b[39m\n",
       "    ),\n",
       "    obs_decoder = Decoder(\n",
       "        output_net = Chain(\n",
       "            layer_1 = Dense(10 => 64, relu),  \u001b[90m# 704 parameters\u001b[39m\n",
       "            layer_2 = Dense(64 => 100),  \u001b[90m# 6_500 parameters\u001b[39m\n",
       "            layer_3 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#45#47\"()),\n",
       "        ),\n",
       "    ),\n",
       "    ctrl_decoder = NoOpLayer(),\n",
       ") \u001b[90m        # Total: \u001b[39m64_140 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m2 states."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs_dim = 100\n",
    "ctrl_dim = 10\n",
    "dev = cpu_device()\n",
    "\n",
    "#Hyperparameters\n",
    "hp = Dict(\"n_states\" => 10, \"hidden_dim\" => 64, \"context_dim\" => 32, \"t_init\" => 50)\n",
    "\n",
    "#Encoder\n",
    "obs_encoder = Recurrent_Encoder(obs_dim, hp[\"n_states\"], hp[\"context_dim\"],  hp[\"hidden_dim\"], hp[\"t_init\"])\n",
    "\n",
    "#Dynamics\n",
    "drift =  ModernWilsonCowan(hp[\"n_states\"], ctrl_dim)\n",
    "drift_aug = Chain(Dense(hp[\"n_states\"] + hp[\"context_dim\"], hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"], tanh))\n",
    "diffusion = Dense(hp[\"n_states\"], hp[\"n_states\"],  sigmoid)\n",
    "dynamics =  SDE(drift, drift_aug, diffusion, EulerHeun(), dt=0.1)\n",
    "\n",
    "#Decoder\n",
    "obs_decoder = MLP_Decoder(hp[\"n_states\"], obs_dim,  hp[\"hidden_dim\"], 1, \"Poisson\")   \n",
    "\n",
    "#Model\n",
    "model = LatentUDE(obs_encoder=obs_encoder, dynamics=dynamics, obs_decoder=obs_decoder, device=dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

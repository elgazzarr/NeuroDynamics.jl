{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering neural dynamics of motor cortex during dealyed reach task using a latent SDE\n",
    "\n",
    "In this example, we will show how to use the latentsde model to infer underlying neural dynamics from single trial spiking recordings of neurons in the dorsal premotor (PMd) and primary motor (M1) cortices.\n",
    "The data is available for download [here](https://dandiarchive.org/#/dandiset/000128).\n",
    "\n",
    "Dynamics in the motor cortext are known to be highly autonomus during simple stereotyped tasks, so it can be predictable given an \"informative\" initial condition even in the absence of stimulus information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg, Revise, Lux, LuxCUDA, Random, DifferentialEquations, SciMLSensitivity, ComponentArrays, Plots, MLUtils, OptimizationOptimisers, LinearAlgebra, Statistics, Printf, PyCall, Distributions, BenchmarkTools, Zygote\n",
    "using IterTools: ncycle\n",
    "using NeuroDynamics\n",
    "np = pyimport(\"numpy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "const dev = device == \"gpu\" ? gpu_device() : cpu_device()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and creating the dataloaders\n",
    "\n",
    "You can prepare the data yourself or use our preprocessed data staright away which is available [here](https://drive.google.com/file/d/1J9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/ahmed.elgazzar/Datasets/NLB/mc_maze.npy\" # change this to the path of the dataset\n",
    "data = np.load(file_path, allow_pickle=true)\n",
    "Y = permutedims(get(data[1], \"spikes\") , [3, 2, 1]) |> Array{Float32}\n",
    "n_neurons , n_timepoints, n_trials = size(Y) \n",
    "ts = range(0, 5.0, length=n_timepoints) |> Array{Float32}\n",
    "Y_trainval , Y_test = splitobs(Y; at=0.8)\n",
    "Y_train , Y_val = splitobs(Y_trainval; at=0.8);\n",
    "train_loader = DataLoader((Y_train, Y_train), batchsize=32, shuffle=true)\n",
    "val_loader = DataLoader((Y_val, Y_val), batchsize=16, shuffle=true)\n",
    "test_loader = DataLoader((Y_test, Y_test), batchsize=16, shuffle=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the model \n",
    "- We will use a \"Recurrent_Encoder\" to infer the initial hidden state from a portion of the observations. \n",
    "- We will use a BlackBox (Neural) SDE with multiplicative noise to model the latent dynamics.\n",
    "- We will use a decoder with a Poisson likelihood to model the spike counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Dict(\"n_states\" => 10, \"hidden_dim\" => 64, \"context_dim\" => 32, \"t_init\" => Int(0.9 * n_timepoints))\n",
    "rng = Random.MersenneTwister(2)\n",
    "obs_encoder = Recurrent_Encoder(n_neurons, hp[\"n_states\"], hp[\"context_dim\"], 32, hp[\"t_init\"])\n",
    "drift =  ModernWilsonCowan(hp[\"n_states\"], 0)\n",
    "drift_aug = Chain(Dense(hp[\"n_states\"] + hp[\"context_dim\"], hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"], tanh))\n",
    "diffusion = Scale(hp[\"n_states\"], sigmoid, init_weight=identity_init(gain=0.1))\n",
    "dynamics =  SDE(drift, drift_aug, diffusion, EulerHeun(), saveat=ts, dt=ts[2]-ts[1]) #ODE(drift, Tsit5)\n",
    "obs_decoder = MLP_Decoder(hp[\"n_states\"], n_neurons, 64, 1, \"Poisson\")   \n",
    "ctrl_encoder, ctrl_decoder = NoOpLayer(), NoOpLayer()\n",
    "model = LatentUDE(obs_encoder, ctrl_encoder, dynamics, obs_decoder, ctrl_decoder, dev)\n",
    "p, st = Lux.setup(rng, model) .|> dev\n",
    "p = p |> ComponentArray{Float32} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model \n",
    "\n",
    "We will train the model using the AdamW optimizer with a learning rate of 1e-3 for 200 epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(model::LatentUDE, p, st, train_loader, val_loader, epochs, print_every)\n",
    "    epoch = 0\n",
    "    L = frange_cycle_linear(epochs+1, 0.5f0, 1.0f0, 1, 0.5)\n",
    "    losses = []\n",
    "    θ_best = nothing\n",
    "    best_metric = -Inf\n",
    "    @info \"Training ....\"\n",
    "\n",
    "    function loss(p, y, _)\n",
    "        y, ts_ = y |> dev, ts |> dev\n",
    "        ŷ, _, x̂₀, kl_path = model(y, nothing, ts_, p, st)\n",
    "        batch_size = size(y)[end]\n",
    "        recon_loss = - poisson_loglikelihood(ŷ, y)/batch_size\n",
    "        kl_init = kl_normal(x̂₀[1], x̂₀[2])\n",
    "        kl_path = mean(kl_path[end,:])\n",
    "        kl_loss =  kl_path  +  kl_init\n",
    "        l =  recon_loss + L[epoch+1]*kl_loss\n",
    "        return l, recon_loss, kl_loss\n",
    "    end\n",
    "\n",
    "\n",
    "    callback = function(opt_state, l, recon_loss , kl_loss)\n",
    "        θ = opt_state.u\n",
    "        push!(losses, l)\n",
    "        if length(losses) % length(train_loader) == 0\n",
    "            epoch += 1\n",
    "        end\n",
    "\n",
    "        if length(losses) % (length(train_loader)*print_every) == 0\n",
    "            @printf(\"Current epoch: %d, Loss: %.2f, PoissonLL: %d, KL: %.2f\\n\", epoch, losses[end], recon_loss, kl_loss)\n",
    "            y, _ = first(val_loader) \n",
    "            ŷ, _, _ = predict(model, y, nothing, ts, θ, st, 20)\n",
    "            ŷₘ = dropdims(mean(ŷ, dims=4), dims=4)\n",
    "            val_bps = bits_per_spike(ŷₘ, y)\n",
    "            @printf(\"Validation bits/spike: %.2f\\n\", val_bps)\n",
    "            if val_bps > best_metric\n",
    "                best_metric = val_bps\n",
    "                 θ_best = copy(θ)\n",
    "                @printf(\"Saving best model\")\n",
    "            end        \n",
    "        end\n",
    "        return false\n",
    "    end\n",
    "\n",
    "    adtype = Optimization.AutoZygote()\n",
    "    optf = OptimizationFunction((p, _ , y, y_) -> loss(p, y, y_), adtype)\n",
    "    optproblem = OptimizationProblem(optf, p)\n",
    "    result = Optimization.solve(optproblem, ADAMW(1e-3), ncycle(train_loader, epochs); callback)\n",
    "    return model, θ_best\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, θ_best = train(model, θ_best, st, train_loader, val_loader, 100, 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = first(test_loader)\n",
    "sample = 24\n",
    "ch = 4\n",
    "ŷ, _, x = predict(model, y, nothing, ts, θ_best, st, 20)\n",
    "ŷₘ = dropdims(mean(ŷ, dims=4), dims=4)\n",
    "ŷₛ = dropdims(std(ŷ, dims=4), dims=4)\n",
    "dist = Poisson.(ŷₘ)\n",
    "pred_spike = rand.(dist)\n",
    "xₘ = dropdims(mean(x, dims=4), dims=4)\n",
    "val_bps = bits_per_spike(ŷₘ, y)\n",
    "\n",
    "p1 = plot(transpose(y[ch:ch,:,sample]), label=\"True Spike\", lw=2)\n",
    "p2 = plot(transpose(pred_spike[ch:ch,:,sample]), label=\"Predicted Spike\", lw=2, color=\"red\")\n",
    "p3 = plot(transpose(ŷₘ[ch:ch,:,sample]), ribbon=transpose(ŷₛ[ch:ch,:,sample]), label=\"Infered rates\", lw=2, color=\"green\", yticks=false)\n",
    "\n",
    "plot(p1, p2,p3, layout=(3,1), size=(800, 400), legend=:topright)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

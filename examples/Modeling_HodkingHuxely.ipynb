{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Hodking-Huxely with latent neural ODEs  \n",
    "\n",
    "In this example will show how to use the latentUDE framework to model a Hodking-Huxely neuron with dynamic synaptic inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg, Revise, Lux, Random, DifferentialEquations, SciMLSensitivity, ComponentArrays, Plots, MLUtils, OptimizationOptimisers, LinearAlgebra, Statistics, Printf\n",
    "using IterTools: ncycle\n",
    "using NeuroDynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.Generating ground truth data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Simulating Synaptic Inputs \n",
    "\n",
    "We will use the [Tsodyks-Markram model](https://www.pnas.org/doi/full/10.1073/pnas.94.2.719) to simulate the synaptic inputs to a neuron. We will generate multiple trajectories to later drive our Hodking-Huxley neuron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 64\n",
    "tspan = (0.0, 500.0)\n",
    "ts = range(tspan[1], tspan[2], length=100)\n",
    "p =  [30, 1000, 50, 0.5, 0.005]\n",
    "function TMS(x, p, t)\n",
    "    v, R, gsyn = x\n",
    "    tau, tau_u, tau_R, v0, gmax = p \n",
    "    dx₁ = -(v / tau_u)\n",
    "    dx₂ = (1 - R) / tau_R\n",
    "    dx₃ = -(gsyn / tau)\n",
    "    return vcat(dx₁, dx₂, dx₃)\n",
    "end\n",
    "\n",
    "function epsp!(integrator)\n",
    "    integrator.u[1] += integrator.p[4] * (1 - integrator.u[1])\n",
    "    integrator.u[3] += integrator.p[5] * integrator.u[1] * integrator.u[2]\n",
    "    integrator.u[2] -= integrator.u[1] * integrator.u[2]\n",
    "end\n",
    "prob = ODEProblem(TMS, [0.0, 1.0, 0.0], tspan, p)\n",
    "function prob_func(prob, i, repeat)\n",
    "    t_start = rand(50:100)\n",
    "    t_int = rand(50:100)\n",
    "    t_end = rand(400:450)\n",
    "    epsp_ts = PresetTimeCallback(t_start:t_int:t_end, epsp!, save_positions=(false, false))\n",
    "    remake(prob, callback=epsp_ts)\n",
    "end\n",
    "\n",
    "ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n",
    "U = solve(ensemble_prob, Tsit5(),  EnsembleThreads(); saveat=ts, trajectories=n_samples);\n",
    "plot(U, vars=(1), alpha=0.5, color=:blue, lw=0.5, legend=false, xlabel=\"Time (ms)\", ylabel=\"Membrane Potential (mV)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Simulating a Hodgkin-Huxley Neuron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potassium ion-channel rate functions\n",
    "alpha_n(v) = (0.02 * (v - 25.0)) / (1.0 - exp((-1.0 * (v - 25.0)) / 9.0))\n",
    "beta_n(v) = (-0.002 * (v - 25.0)) / (1.0 - exp((v - 25.0) / 9.0))\n",
    "\n",
    "# Sodium ion-channel rate functions\n",
    "alpha_m(v) = (0.182 * (v + 35.0)) / (1.0 - exp((-1.0 * (v + 35.0)) / 9.0))\n",
    "beta_m(v) = (-0.124 * (v + 35.0)) / (1.0 - exp((v + 35.0) / 9.0))\n",
    "\n",
    "alpha_h(v) = 0.25 * exp((-1.0 * (v + 90.0)) / 12.0)\n",
    "beta_h(v) = (0.25 * exp((v + 62.0) / 6.0)) / exp((v + 90.0) / 12.0)\n",
    "\n",
    "\n",
    "\n",
    "function HH(x, p, t, u)\n",
    "    gK, gNa, gL, EK, ENa, EL, C, ESyn, i = p\n",
    "    v, n, m, h = x\n",
    "    ISyn(t) = u[i](t)[end] * (ESyn - v)\n",
    "\n",
    "    dx₁ = ((gK * (n^4.0) * (EK - v)) + (gNa * (m^3.0) * h * (ENa - v)) + (gL * (EL - v)) + ISyn(t)) / C\n",
    "    dx₂ = (alpha_n(v) * (1.0 - n)) - (beta_n(v) * n)\n",
    "    dx₃ = (alpha_m(v) * (1.0 - m)) - (beta_m(v) * m)\n",
    "    dx₄ = (alpha_h(v) * (1.0 - h)) - (beta_h(v) * h)\n",
    "\n",
    "    dx = vcat(dx₁, dx₂, dx₃, dx₄)\n",
    "end\n",
    "\n",
    "dxdt(x, p, t) = HH(x, p, t, U)\n",
    "\n",
    "p = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0, 1] \n",
    "# n, m & h steady-states\n",
    "n_inf(v) = alpha_n(v) / (alpha_n(v) + beta_n(v))\n",
    "m_inf(v) = alpha_m(v) / (alpha_m(v) + beta_m(v))\n",
    "h_inf(v) = alpha_h(v) / (alpha_h(v) + beta_h(v))\n",
    "\n",
    "v0 = -60\n",
    "x0 = [v0, n_inf(v0), m_inf(v0), h_inf(v0)]\n",
    "prob = ODEProblem(dxdt, x0, tspan, p)\n",
    "prob_func(prob, i, repeat) = remake(prob, p=(p[1:end-1]..., i))\n",
    "ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n",
    "Y = solve(ensemble_prob, EnsembleThreads(); saveat=ts, trajectories=n_samples)\n",
    "plot(Y, vars=1, label=\"v\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating a dataset and splitting it into train val test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data = Array(Y) .|> Float32\n",
    "U_data = Array(U) .|> Float32\n",
    "input_dim = size(U_data)[1]\n",
    "obs_dim = size(Y_data)[1]\n",
    "(u_train, y_train), (u_val, y_val) = splitobs((U_data, Y_data); at=0.8, shuffle=true)\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader((U_data, Y_data), batchsize=32, shuffle=false)\n",
    "val_loader = DataLoader((U_data, Y_data), batchsize=32, shuffle=true);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_model(n_states, ctrl_dim, obs_dim, context_dim, t_init)\n",
    "    rng = Random.MersenneTwister(1234)\n",
    "    obs_encoder = Recurrent_Encoder(obs_dim, n_states, context_dim, 32, t_init)\n",
    "    vector_field = Chain(Dense(n_states+ctrl_dim, 32, softplus), Dense(32, n_states, tanh))\n",
    "    dynamics = ODE(vector_field, Euler(); saveat=ts, dt=2.0)\n",
    "    obs_decoder = Linear_Decoder(n_states, obs_dim, \"None\")   \n",
    "\n",
    "    model = LatentUDE(obs_encoder=obs_encoder, dynamics=dynamics, obs_decoder=obs_decoder)\n",
    "    p, st = Lux.setup(rng, model)\n",
    "    p = p |> ComponentArray{Float32}\n",
    "    return model, p, st\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "context_dim = 0 # No need for context if we have ODE dynamics\n",
    "t_init = 50\n",
    "model, p, st = create_model(latent_dim, input_dim, obs_dim, context_dim, t_init)\n",
    "u, y = first(train_loader)\n",
    "ts = ts |> Array{Float32};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model via variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(model, p, st, train_loader, val_loader, epochs, print_every)\n",
    "    \n",
    "    epoch = 0\n",
    "    L = frange_cycle_linear(epochs+1, 0.0f0, 1.0f0, 1, 0.5)\n",
    "    losses = []\n",
    "    best_model_params = nothing\n",
    "    best_metric = Inf\n",
    "    function loss(p, u, y, ts=ts)\n",
    "        ŷ, û, x̂₀, _ = model(y, u, ts, p, st)\n",
    "        batch_size = size(y)[end]\n",
    "        recon_loss = mse(ŷ[1:1, :, :], y[1:1, :, :])/batch_size\n",
    "        kl_loss = kl_normal(x̂₀[1], x̂₀[2])/batch_size\n",
    "        l =  0.1*recon_loss + L[epoch+1]*kl_loss\n",
    "        return l, recon_loss, kl_loss\n",
    "    end\n",
    "\n",
    "\n",
    "    callback = function(opt_state, l, recon_loss, kl_loss)\n",
    "        θ = opt_state.u\n",
    "        push!(losses, l)\n",
    "        if length(losses) % length(train_loader) == 0\n",
    "            epoch += 1\n",
    "        end\n",
    "\n",
    "        if length(losses) % (length(train_loader)*print_every) == 0\n",
    "            @printf(\"Current epoch: %d, Loss: %.2f, Reconstruction: %d, KL: %.2f\\n\", epoch, losses[end], recon_loss, kl_loss)\n",
    "            u, y = first(val_loader)\n",
    "            batch_size = size(y)[end]\n",
    "            ŷ, _, x = predict(model, y, u, ts, θ, st, 20)\n",
    "            ŷ_mean = dropdims(mean(ŷ, dims=4), dims=4)\n",
    "            val_mse = mse(ŷ_mean[1:1, :, :], y[1:1, :, :])\n",
    "            @printf(\"Validation MSE: %.2f\\n\", val_mse)\n",
    "            if val_mse < best_metric\n",
    "                best_metric = val_mse\n",
    "                @printf(\"Saving model with best metric: %.2f\\n\", best_metric)\n",
    "                best_model_params = copy(θ)\n",
    "\n",
    "            end\n",
    "\n",
    "            pl = plot(transpose(y[1:1, :, 1]), label=\"True\", lw=2.0)\n",
    "            plot!(pl, transpose(ŷ_mean[1:1, :, 1]), label=\"Predicted\", lw=2.0, xlabel=\"Time (ms)\", ylabel=\"Membrane Potential (mV)\")\n",
    "            display(pl)\n",
    "        \n",
    "        end\n",
    "        return false\n",
    "    end\n",
    "\n",
    "    adtype = Optimization.AutoZygote()\n",
    "    optf = OptimizationFunction((p, _ , u, y) -> loss(p, u, y), adtype)\n",
    "    optproblem = OptimizationProblem(optf, p)\n",
    "    result = Optimization.solve(optproblem, ADAMW(1e-3), ncycle(train_loader, epochs); callback)\n",
    "    return result, losses, model, best_model_params\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, losses, model, best_p = train(model, p, st, train_loader, val_loader, 5000, 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

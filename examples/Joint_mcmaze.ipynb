{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint modeling of neural and behavioural dynamics during dealyed reach task\n",
    "\n",
    "In this example, we will show how to use the latentsde model to generate neural observations (spiking recordings of neurons in the dorsal premotor (PMd) and primary motor (M1) cortices) and behavioural observations (Hand velocity) of a monkey doing a dealyed reach task. \n",
    "The data is available for download [here](https://dandiarchive.org/#/dandiset/000128).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg, Revise, Lux, Random, DifferentialEquations, SciMLSensitivity, ComponentArrays, Plots, MLUtils, OptimizationOptimisers, LinearAlgebra, Statistics, Printf, PyCall, Distributions\n",
    "using IterTools: ncycle\n",
    "using NeuroDynamics\n",
    "np = pyimport(\"numpy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data and creating the dataloaders\n",
    "\n",
    "You can prepare the data yourself or use our preprocessed data staright away which is available [here](https://drive.google.com/file/d/1J9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/ahmed.elgazzar/Code/Rhythms/Datasets/NLB/mc_maze.npy\"\n",
    "data = np.load(file_path, allow_pickle=true)\n",
    "Y_neural = permutedims(get(data[1], \"spikes\") , [3, 2, 1])[:,:,1:50]\n",
    "Y_behaviour = permutedims(get(data[1], \"hand_vel\") , [3, 2, 1])[:,:,1:50]\n",
    "n_neurons = size(Y_neural)[1]\n",
    "n_neurons , n_timepoints, n_trials = size(Y_neural);\n",
    "n_behviour = size(Y_behaviour)[1]\n",
    "ts = range(0, 4, length=n_timepoints);\n",
    "ts_input = repeat(ts, 1, n_trials) \n",
    "U = reshape(ts_input, (1, size(ts_input)...)) # We use time as our control input to the model\n",
    "n_ctrl = size(U)[1]\n",
    "(U_train, Yn_train, Yb_train) , (U_test, Yn_test, Yb_test) = splitobs((U, Y_neural, Y_behaviour); at=0.8)\n",
    "train_loader = DataLoader((U_train, Yn_train, Yb_train), batchsize=32, shuffle=true)\n",
    "val_loader = DataLoader((U_test, Yn_test, Yb_test), batchsize=10, shuffle=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the model \n",
    "- We will use a \"Recurrent_Encoder\" to infer the initial hidden state from a portion of the observations. \n",
    "- We will use a BlackBox (Neural) SDE with multiplicative noise to model the latent dynamics.\n",
    "- We will use a multi-headed decoder, one for the neural observations and one for behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Dict(\"n_states\" => 16, \"hidden_dim\" => 64, \"context_dim\" => 32, \"t_init\" => Int(0.8 * n_timepoints))\n",
    "rng = Random.MersenneTwister(1234)\n",
    "obs_encoder = Recurrent_Encoder(n_neurons, hp[\"n_states\"], hp[\"context_dim\"], 32, hp[\"t_init\"])\n",
    "drift = ModernWilsonCowan(hp[\"n_states\"], n_ctrl)   #Chain(Dense(hp[\"n_states\"], hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"], tanh))\n",
    "drift_aug = Chain(Dense(hp[\"n_states\"] + hp[\"context_dim\"] + n_ctrl, hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"],tanh))\n",
    "diffusion = Scale(hp[\"n_states\"], sigmoid, init_weight=identity_init(gain=0.1))\n",
    "dynamics = SDE(drift, drift_aug, diffusion, EulerHeun(), saveat=ts, dt=ts[2]-ts[1])\n",
    "obs_decoder = Chain(MLP_Decoder(hp[\"n_states\"], n_neurons, 64, 1, \"Poisson\"), Lux.BranchLayer(NoOpLayer(), Linear_Decoder(n_neurons, n_behviour,\"Gaussian\")))\n",
    "\n",
    "ctrl_encoder, ctrl_decoder = NoOpLayer(), NoOpLayer()\n",
    "model = LatentUDE(obs_encoder, ctrl_encoder, dynamics, obs_decoder, ctrl_decoder)\n",
    "p, st = Lux.setup(rng, model)\n",
    "p = p |> ComponentArray{Float32};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model \n",
    "\n",
    "We will train the model using the AdamW optimizer with a learning rate of 1e-3 for 200 epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(model::LatentUDE, p, st, train_loader, val_loader, epochs, print_every)\n",
    "    \n",
    "    epoch = 0\n",
    "    L = frange_cycle_linear(epochs+1, 0.5f0, 1.0f0, 1, 0.3)\n",
    "    losses = []\n",
    "    θ_best = nothing\n",
    "    best_metric = -Inf\n",
    "    println(\"Training ...\")\n",
    "\n",
    "    function loss(p, u, y_n, y_b)\n",
    "        (ŷ_n, ŷ_b), _, x̂₀, kl_path = model(y_n, u, ts, p, st)\n",
    "        batch_size = size(y_n)[end]\n",
    "        neural_loss = - poisson_loglikelihood(ŷ_n, y_n)/batch_size\n",
    "        behaviorual_loss = - normal_loglikelihood(ŷ_b..., y_b)\n",
    "        obs_loss = neural_loss + behaviorual_loss\n",
    "        kl_init = kl_normal(x̂₀[1], x̂₀[2])\n",
    "        kl_path = mean(kl_path[end,:])\n",
    "        kl_loss =  kl_path  +  kl_init\n",
    "        l =  0.1*obs_loss + 10*L[epoch+1]*kl_loss\n",
    "        return l, obs_loss, kl_loss\n",
    "    end\n",
    "\n",
    "\n",
    "    callback = function(opt_state, l, obs_loss , kl_loss)\n",
    "        θ = opt_state.u\n",
    "        push!(losses, l)\n",
    "        if length(losses) % length(train_loader) == 0\n",
    "            epoch += 1\n",
    "        end\n",
    "\n",
    "        if length(losses) % (length(train_loader)*print_every) == 0\n",
    "            @printf(\"Current epoch: %d, Loss: %.2f, Observations_loss: %d, KL: %.2f\\n\", epoch, losses[end], obs_loss, kl_loss)\n",
    "            u, y_n, y_b = first(val_loader) \n",
    "            (ŷ_n, ŷ_b), _, _ = predict(model, y_n, u, ts, θ, st, 20) \n",
    "            ŷ_n = dropdims(mean(ŷ_n, dims=4), dims=4)\n",
    "            ŷ_b_m, ŷ_b_s = dropdims(mean(ŷ_b[1], dims=4), dims=4), dropdims(mean(ŷ_b[2], dims=4), dims=4)\n",
    "            val_bps = bits_per_spike(ŷ_n, y_n)\n",
    "            val_ll = normal_loglikelihood(ŷ_b_m, ŷ_b_s, y_b)\n",
    "            @printf(\"Validation bits/spike: %.2f\\n\", val_bps)\n",
    "            @printf(\"Validation behaviour log-likelihood: %.2f\\n\", val_ll)\n",
    "            if val_ll > best_metric\n",
    "                best_metric = val_ll\n",
    "                 θ_best = copy(θ)\n",
    "                 @printf(\"**** Saving best model ****\\n\")\n",
    "                end   \n",
    "            d = plot_preds(y_b,  ŷ_b[1])\n",
    "            display(d)\n",
    "\n",
    "        end\n",
    "        return false\n",
    "    end\n",
    "\n",
    "    adtype = Optimization.AutoZygote()\n",
    "    optf = OptimizationFunction((p, _ , u, y_n, y_b) -> loss(p, u, y_n, y_b), adtype)\n",
    "    optproblem = OptimizationProblem(optf, p)\n",
    "    result = Optimization.solve(optproblem, ADAMW(1e-3), ncycle(train_loader, epochs); callback)\n",
    "    return model, θ_best\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, θ_best = train(model, θ_best, st, train_loader, val_loader, 5, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, y_n, y_b = first(train_loader) \n",
    "(ŷ_n, ŷ_b), _, x = predict(model, y_n, u, ts, θ_best, st, 20)\n",
    "sample = 5\n",
    "ch = 15\n",
    "ŷₘ = dropmean(ŷ_n, dims=4)\n",
    "ŷₛ = dropmean(ŷ_n, dims=4)\n",
    "dist = Poisson.(ŷₘ)\n",
    "pred_spike = rand.(dist)\n",
    "xₘ = dropmean(x, dims=4)\n",
    "val_bps = bits_per_spike(ŷₘ, y_n)\n",
    "\n",
    "p1 = plot(transpose(y_n[ch:ch,:,sample]), label=\"True Spike\", lw=2)\n",
    "p2 = plot(transpose(pred_spike[ch:ch,:,sample]), label=\"Predicted Spike\", lw=2, color=\"red\")\n",
    "p3 = plot(transpose(ŷₘ[ch:ch,:,sample]), ribbon=transpose(ŷₛ[ch:ch,:,sample]), label=\"Infered rates\", lw=2, color=\"green\")\n",
    "@printf(\"Validation bits/spike: %.2f\\n\", val_bps)\n",
    "\n",
    "plot(p1, p2,p3, layout=(3,1), size=(800, 400), legend=:topleft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = rand(1 , 2, 5)\n",
    "ys = rand(1 , 2, 5)\n",
    "\n",
    "for (x, y) in zip(eachslice(xs, dims=1), eachslice(ys, dims=1))\n",
    "    println(size(x))\n",
    "    println(size(y))\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 15\n",
    "plot_samples(ŷ_b[1], s)\n",
    "plot!(transpose(y_b[:,:,s]), label=\"True Behaviour\", lw=2, color=\"red\", legend=:topleft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate_timeseries(y_b)\n",
    "gif(anim, \"NLB_LatentUDE_behaviour2.gif\", fps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate_timeseries(ŷ_b[1][:,:,:,4])\n",
    "gif(anim, \"NLB_LatentUDE_behaviour2.gif\", fps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ, _, x = predict(model, y, nothing, ts, θ_best, st, 20)\n",
    "ŷₘ = dropdims(mean(ŷ, dims=4), dims=4)\n",
    "\n",
    "xₘ = dropdims(mean(x, dims=4), dims=4)\n",
    "val_bps = bits_per_spike(ŷₘ, y)\n",
    "plot(transpose(x[:,:,sample,1]), legend=false, title=\"Predicted states\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

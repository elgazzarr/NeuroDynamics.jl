var documenterSearchIndex = {"docs":
[{"location":"examples/Neural_mcmaze/#Infering-neural-dynamics-of-motor-cortex-during-dealyed-reach-task-using-a-latent-SDE","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics of motor cortex during dealyed reach task using a latent SDE","text":"","category":"section"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"In this example, we will show how to use the latentsde model to infer underlying neural dynamics from single trial spiking recordings of neurons in the dorsal premotor (PMd) and primary motor (M1) cortices. The data is available for download here.","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"Dynamics in the motor cortext are known to be highly autonomus during simple stereotyped tasks, so it can be predictable given an \"informative\" initial condition even in the absence of stimulus information. ","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"using Pkg, Revise, Lux, LuxCUDA, Random, DifferentialEquations, SciMLSensitivity, ComponentArrays, Plots, MLUtils, OptimizationOptimisers, LinearAlgebra, Statistics, Printf, PyCall, Distributions, BenchmarkTools, Zygote\nusing IterTools: ncycle\nusing NeuroDynamics\nnp = pyimport(\"numpy\");","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"device = \"cpu\"\nconst dev = device == \"gpu\" ? gpu_device() : cpu_device()\n","category":"page"},{"location":"examples/Neural_mcmaze/#1.-Loading-the-data-and-creating-the-dataloaders","page":"Infering neural dynamics in motor cortex","title":"1. Loading the data and creating the dataloaders","text":"","category":"section"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"You can prepare the data yourself or use our preprocessed data staright away which is available here","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"file_path = \"/Users/ahmed.elgazzar/Datasets/NLB/mc_maze.npy\" # change this to the path of the dataset\ndata = np.load(file_path, allow_pickle=true)\nY = permutedims(get(data[1], \"spikes\") , [3, 2, 1]) |> Array{Float32}\nn_neurons , n_timepoints, n_trials = size(Y) \nts = range(0, 5.0, length=n_timepoints) |> Array{Float32}\nY_trainval , Y_test = splitobs(Y; at=0.8)\nY_train , Y_val = splitobs(Y_trainval; at=0.8);\ntrain_loader = DataLoader((Y_train, Y_train), batchsize=32, shuffle=true)\nval_loader = DataLoader((Y_val, Y_val), batchsize=16, shuffle=true)\ntest_loader = DataLoader((Y_test, Y_test), batchsize=16, shuffle=true);","category":"page"},{"location":"examples/Neural_mcmaze/#2.-Defining-the-model","page":"Infering neural dynamics in motor cortex","title":"2. Defining the model","text":"","category":"section"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"We will use a \"Recurrent_Encoder\" to infer the initial hidden state from a portion of the observations. \nWe will use a BlackBox (Neural) SDE with multiplicative noise to model the latent dynamics.\nWe will use a decoder with a Poisson likelihood to model the spike counts.","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"hp = Dict(\"n_states\" => 10, \"hidden_dim\" => 64, \"context_dim\" => 32, \"t_init\" => Int(0.9 * n_timepoints))\nrng = Random.MersenneTwister(2)\nobs_encoder = Recurrent_Encoder(n_neurons, hp[\"n_states\"], hp[\"context_dim\"], 32, hp[\"t_init\"])\ndrift =  ModernWilsonCowan(hp[\"n_states\"], 0)\ndrift_aug = Chain(Dense(hp[\"n_states\"] + hp[\"context_dim\"], hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"], tanh))\ndiffusion = Scale(hp[\"n_states\"], sigmoid, init_weight=identity_init(gain=0.1))\ndynamics =  SDE(drift, drift_aug, diffusion, EulerHeun(), saveat=ts, dt=ts[2]-ts[1]) #ODE(drift, Tsit5)\nobs_decoder = MLP_Decoder(hp[\"n_states\"], n_neurons, 64, 1, \"Poisson\")   \nctrl_encoder, ctrl_decoder = NoOpLayer(), NoOpLayer()\nmodel = LatentUDE(obs_encoder, ctrl_encoder, dynamics, obs_decoder, ctrl_decoder, dev)\np, st = Lux.setup(rng, model) .|> dev\np = p |> ComponentArray{Float32} ","category":"page"},{"location":"examples/Neural_mcmaze/#3.-Training-the-model","page":"Infering neural dynamics in motor cortex","title":"3. Training the model","text":"","category":"section"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"We will train the model using the AdamW optimizer with a learning rate of 1e-3 for 200 epochs. ","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"function train(model::LatentUDE, p, st, train_loader, val_loader, epochs, print_every)\n    epoch = 0\n    L = frange_cycle_linear(epochs+1, 0.5f0, 1.0f0, 1, 0.5)\n    losses = []\n    θ_best = nothing\n    best_metric = -Inf\n    @info \"Training ....\"\n\n    function loss(p, y, _)\n        y, ts_ = y |> dev, ts |> dev\n        ŷ, _, x̂₀, kl_path = model(y, nothing, ts_, p, st)\n        batch_size = size(y)[end]\n        recon_loss = - poisson_loglikelihood(ŷ, y)/batch_size\n        kl_init = kl_normal(x̂₀[1], x̂₀[2])\n        kl_path = mean(kl_path[end,:])\n        kl_loss =  kl_path  +  kl_init\n        l =  recon_loss + L[epoch+1]*kl_loss\n        return l, recon_loss, kl_loss\n    end\n\n\n    callback = function(opt_state, l, recon_loss , kl_loss)\n        θ = opt_state.u\n        push!(losses, l)\n        if length(losses) % length(train_loader) == 0\n            epoch += 1\n        end\n\n        if length(losses) % (length(train_loader)*print_every) == 0\n            @printf(\"Current epoch: %d, Loss: %.2f, PoissonLL: %d, KL: %.2f\\n\", epoch, losses[end], recon_loss, kl_loss)\n            y, _ = first(val_loader) \n            ŷ, _, _ = predict(model, y, nothing, ts, θ, st, 20)\n            ŷₘ = dropdims(mean(ŷ, dims=4), dims=4)\n            val_bps = bits_per_spike(ŷₘ, y)\n            @printf(\"Validation bits/spike: %.2f\\n\", val_bps)\n            if val_bps > best_metric\n                best_metric = val_bps\n                 θ_best = copy(θ)\n                @printf(\"Saving best model\")\n            end        \n        end\n        return false\n    end\n\n    adtype = Optimization.AutoZygote()\n    optf = OptimizationFunction((p, _ , y, y_) -> loss(p, y, y_), adtype)\n    optproblem = OptimizationProblem(optf, p)\n    result = Optimization.solve(optproblem, ADAMW(1e-3), ncycle(train_loader, epochs); callback)\n    return model, θ_best\n    \nend\n","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"model, θ_best = train(model, θ_best, st, train_loader, val_loader, 100, 10);","category":"page"},{"location":"examples/Neural_mcmaze/","page":"Infering neural dynamics in motor cortex","title":"Infering neural dynamics in motor cortex","text":"y, _ = first(test_loader)\nsample = 24\nch = 4\nŷ, _, x = predict(model, y, nothing, ts, θ_best, st, 20)\nŷₘ = dropdims(mean(ŷ, dims=4), dims=4)\nŷₛ = dropdims(std(ŷ, dims=4), dims=4)\ndist = Poisson.(ŷₘ)\npred_spike = rand.(dist)\nxₘ = dropdims(mean(x, dims=4), dims=4)\nval_bps = bits_per_spike(ŷₘ, y)\n\np1 = plot(transpose(y[ch:ch,:,sample]), label=\"True Spike\", lw=2)\np2 = plot(transpose(pred_spike[ch:ch,:,sample]), label=\"Predicted Spike\", lw=2, color=\"red\")\np3 = plot(transpose(ŷₘ[ch:ch,:,sample]), ribbon=transpose(ŷₛ[ch:ch,:,sample]), label=\"Infered rates\", lw=2, color=\"green\", yticks=false)\n\nplot(p1, p2,p3, layout=(3,1), size=(800, 400), legend=:topright)\n","category":"page"},{"location":"tutorials/2-building_latentUDE/#Creating-a-Latent-SDE-with-differentiable-drift-and-diffusion-functions","page":"Constructing a LatentSDE","title":"Creating a Latent SDE with differentiable drift and diffusion functions","text":"","category":"section"},{"location":"tutorials/2-building_latentUDE/","page":"Constructing a LatentSDE","title":"Constructing a LatentSDE","text":"using NeuroDynamics, Lux, Random, Plots, DifferentialEquations, ComponentArrays","category":"page"},{"location":"tutorials/2-building_latentUDE/","page":"Constructing a LatentSDE","title":"Constructing a LatentSDE","text":"Below we will create an example forced latent SDE with differentiable drift and diffusion functions.","category":"page"},{"location":"tutorials/2-building_latentUDE/","page":"Constructing a LatentSDE","title":"Constructing a LatentSDE","text":"For the encoder, we will use a Recurrent_Encoder which will take the input sequence and return the hidden state of the RNN at the last time step. This hidden state will be used as the initial condition for the SDE solver. It will also return a context vector which will be used to condition augmented SDE. ","category":"page"},{"location":"tutorials/2-building_latentUDE/","page":"Constructing a LatentSDE","title":"Constructing a LatentSDE","text":"The generative SDE will be defined with a ModernWilsonCowan drift and a 1 layer network for the diffusion. The augmented SDE will have an MLP for the drift and share the same diffusion with the generative SDE. ","category":"page"},{"location":"tutorials/2-building_latentUDE/","page":"Constructing a LatentSDE","title":"Constructing a LatentSDE","text":"The decoder is an MLP with Poisson noise. ","category":"page"},{"location":"tutorials/2-building_latentUDE/","page":"Constructing a LatentSDE","title":"Constructing a LatentSDE","text":"obs_dim = 100\nctrl_dim = 10\ndev = cpu_device()\n\n#Hyperparameters\nhp = Dict(\"n_states\" => 10, \"hidden_dim\" => 64, \"context_dim\" => 32, \"t_init\" => 50)\n\n#Encoder\nobs_encoder = Recurrent_Encoder(obs_dim, hp[\"n_states\"], hp[\"context_dim\"],  hp[\"hidden_dim\"], hp[\"t_init\"])\n\n#Dynamics\ndrift =  ModernWilsonCowan(hp[\"n_states\"], ctrl_dim)\ndrift_aug = Chain(Dense(hp[\"n_states\"] + hp[\"context_dim\"], hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"], tanh))\ndiffusion = Dense(hp[\"n_states\"], hp[\"n_states\"],  sigmoid)\ndynamics =  SDE(drift, drift_aug, diffusion, EulerHeun(), dt=0.1)\n\n#Decoder\nobs_decoder = MLP_Decoder(hp[\"n_states\"], obs_dim,  hp[\"hidden_dim\"], 1, \"Poisson\")   \n\n#Model\nmodel = LatentUDE(obs_encoder=obs_encoder, dynamics=dynamics, obs_decoder=obs_decoder, device=dev)","category":"page"},{"location":"tutorials/2-building_latentUDE/","page":"Constructing a LatentSDE","title":"Constructing a LatentSDE","text":"LatentUDE(\n    obs_encoder = Encoder(\n        linear_net = Dense(100 => 64),  \u001b[90m# 6_464 parameters\u001b[39m\n        init_net = Chain(\n            layer_1 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#34#37\"{Int64}(50)),\n            layer_2 = Recurrence(\n                cell = LSTMCell(64 => 64),  \u001b[90m# 33_024 parameters\u001b[39m\u001b[90m, plus 1\u001b[39m\n            ),\n            layer_3 = BranchLayer(\n                layer_1 = Dense(64 => 10),  \u001b[90m# 650 parameters\u001b[39m\n                layer_2 = Dense(64 => 10, softplus),  \u001b[90m# 650 parameters\u001b[39m\n            ),\n        ),\n        context_net = Chain(\n            layer_1 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#35#38\"()),\n            layer_2 = Recurrence(\n                cell = LSTMCell(64 => 32),  \u001b[90m# 12_416 parameters\u001b[39m\u001b[90m, plus 1\u001b[39m\n            ),\n            layer_3 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#36#39\"()),\n        ),\n    ),\n    ctrl_encoder = NoOpLayer(),\n    dynamics = SDE(\n        drift = ModernWilsonCowan(10, 10, WeightInitializers.ones32, WeightInitializers.glorot_uniform, WeightInitializers.glorot_uniform, WeightInitializers.ones32),  \u001b[90m# 220 parameters\u001b[39m\n        drift_aug = Chain(\n            layer_1 = Dense(42 => 64, softplus),  \u001b[90m# 2_752 parameters\u001b[39m\n            layer_2 = Dense(64 => 10, tanh_fast),  \u001b[90m# 650 parameters\u001b[39m\n        ),\n        diffusion = Dense(10 => 10, sigmoid_fast),  \u001b[90m# 110 parameters\u001b[39m\n    ),\n    obs_decoder = Decoder(\n        output_net = Chain(\n            layer_1 = Dense(10 => 64, relu),  \u001b[90m# 704 parameters\u001b[39m\n            layer_2 = Dense(64 => 100),  \u001b[90m# 6_500 parameters\u001b[39m\n            layer_3 = WrappedFunction{:direct_call}(NeuroDynamics.var\"#45#47\"()),\n        ),\n    ),\n    ctrl_decoder = NoOpLayer(),\n) \u001b[90m        # Total: \u001b[39m64_140 parameters,\n\u001b[90m          #        plus \u001b[39m2 states.","category":"page"},{"location":"examples/Joint_area2bump/#Joint-modeling-of-neural-and-behavioural-dynamics-during-perturbed-center-out-reaches","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Joint modeling of neural and behavioural dynamics during perturbed center-out reaches","text":"","category":"section"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"In this example, we will show how to use a forced latentsde model to generate neural observations (spiking recordings in Area 2) and behavioural observations (Hand position) of a monkey doing a center-out reach task. In some of the trials the monkey is perturbed by a force field. The force field is provided as an input to the model. ","category":"page"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"The data is available for download here.","category":"page"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"using Pkg, Revise, Lux, LuxCUDA, CUDA, Random, DifferentialEquations, SciMLSensitivity, ComponentArrays, Plots, MLUtils, OptimizationOptimisers, LinearAlgebra, Statistics, Printf, PyCall, Distributions\nusing IterTools: ncycle\nusing NeuroDynamics\nnp = pyimport(\"numpy\")\ndevice = \"cpu\"\nconst dev = device == \"gpu\" ? gpu_device() : cpu_device()\n","category":"page"},{"location":"examples/Joint_area2bump/#1.-Loading-the-data-and-creating-the-dataloaders","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"1. Loading the data and creating the dataloaders","text":"","category":"section"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"You can prepare the data yourself or use our preprocessed data staright away which is available here","category":"page"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"file_path = \"/Users/ahmed.elgazzar/Datasets/NLB/area2_bump.npy\" # change this to the path to the dataset\ndata = np.load(file_path, allow_pickle=true)\nU = permutedims(get(data[1], \"force\") , [3, 2, 1])\nY_neural = permutedims(get(data[1], \"spikes\") , [3, 2, 1])\nY_behaviour = permutedims(get(data[1], \"hand_pos\") , [3, 2, 1])\nn_neurons , n_timepoints, n_trials = size(Y_neural);\nn_behviour = size(Y_behaviour)[1] \nn_control = size(U)[1]\nts = range(0, 4, length=n_timepoints);\n(U_train, Yn_train, Yb_train) , (U_test, Yn_test, Yb_test) = splitobs((U, Y_neural, Y_behaviour); at=0.8)\ntrain_loader = DataLoader((U_train, Yn_train, Yb_train), batchsize=32, shuffle=true)\nval_loader = DataLoader((U_test, Yn_test, Yb_test), batchsize=10, shuffle=true);","category":"page"},{"location":"examples/Joint_area2bump/#2.-Defining-the-model","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"2. Defining the model","text":"","category":"section"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"We will use a \"Recurrent_Encoder\" to infer the initial hidden state from a portion of the observations. \nWe will use a modern intepertation of the Wilson Cowan model with multiplicative noise to model the latent dynamics.\nWe will use a multi-headed decoder, one for the neural observations and one for behaviour.","category":"page"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"hp = Dict(\"n_states\" => 16, \"hidden_dim\" => 64, \"context_dim\" => 32, \"t_init\" => Int(0.5 * n_timepoints))\nrng = Random.MersenneTwister(1234)\nobs_encoder = Recurrent_Encoder(n_neurons, hp[\"n_states\"], hp[\"context_dim\"], 32, hp[\"t_init\"])\ndrift = ModernWilsonCowan(hp[\"n_states\"], n_control)\ndrift_aug = Chain(Dense(hp[\"n_states\"] + hp[\"context_dim\"] + n_control, hp[\"hidden_dim\"], relu), Dense(hp[\"hidden_dim\"], hp[\"n_states\"],tanh))\ndiffusion = Scale(hp[\"n_states\"], sigmoid)\ndynamics = SDE(drift, drift_aug, diffusion, EulerHeun(), saveat=ts, dt=ts[2]-ts[1])\nobs_decoder = Chain(MLP_Decoder(hp[\"n_states\"], n_neurons, 64, 1, \"Poisson\"), Lux.BranchLayer(NoOpLayer(), Linear_Decoder(n_neurons, n_behviour,\"Gaussian\")))\nctrl_encoder, ctrl_decoder = NoOpLayer(), NoOpLayer()\nmodel = LatentUDE(obs_encoder, ctrl_encoder, dynamics, obs_decoder, ctrl_decoder, dev)\n\np, st = Lux.setup(rng, model)\np = p |> ComponentArray{Float32};\n","category":"page"},{"location":"examples/Joint_area2bump/#3.-Training-the-model","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"3. Training the model","text":"","category":"section"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"We will train the model using the AdamW optimizer with a learning rate of 1e-3 for 200 epochs. ","category":"page"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"function train(model::LatentUDE, p, st, train_loader, val_loader, epochs, print_every)\n    \n    epoch = 0\n    L = frange_cycle_linear(epochs+1, 0.5f0, 1.0f0, 1, 0.5)\n    losses = []\n    θ_best = nothing\n    best_metric = -Inf\n    println(\"Training ...\")\n\n    function loss(p, u, y_n, y_b)\n        (ŷ_n, ŷ_b), _, x̂₀, kl_path = model(y_n, u, ts, p, st)\n        batch_size = size(y_n)[end]\n        neural_loss = - poisson_loglikelihood(ŷ_n, y_n)/batch_size\n        behaviorual_loss = - normal_loglikelihood(ŷ_b..., y_b)\n        obs_loss = neural_loss + behaviorual_loss\n        kl_init = kl_normal(x̂₀[1], x̂₀[2])\n        kl_path = mean(kl_path[end,:])\n        kl_loss =  kl_path  +  kl_init\n        l =  0.5*obs_loss + L[epoch+1]*kl_loss\n        return l, obs_loss, kl_loss\n    end\n\n\n    callback = function(opt_state, l, obs_loss , kl_loss)\n        θ = opt_state.u\n        push!(losses, l)\n        if length(losses) % length(train_loader) == 0\n            epoch += 1\n        end\n\n        if length(losses) % (length(train_loader)*print_every) == 0\n            @printf(\"Current epoch: %d, Loss: %.2f, Observations_loss: %d, KL: %.2f\\n\", epoch, losses[end], obs_loss, kl_loss)\n            u, y_n, y_b = first(val_loader) \n            (ŷ_n, ŷ_b), _, _ = predict(model, y_n, u, ts, θ, st, 20) \n            ŷ_n = dropdims(mean(ŷ_n, dims=4), dims=4)\n            ŷ_b_m, ŷ_b_s = dropdims(mean(ŷ_b[1], dims=4), dims=4), dropdims(mean(ŷ_b[2], dims=4), dims=4)\n            val_bps = bits_per_spike(ŷ_n, y_n)\n            val_ll = normal_loglikelihood(ŷ_b_m, ŷ_b_s, y_b)\n            @printf(\"Validation bits/spike: %.2f\\n\", val_bps)\n            @printf(\"Validation behaviour log-likelihood: %.2f\\n\", val_ll)\n            if val_ll > best_metric\n                best_metric = val_ll\n                 θ_best = copy(θ)\n                @printf(\"Saving best model \\n\")\n            end   \n            d = plot_ci(y_b,  ŷ_b..., 1.96)\n            display(d)\n\n        end\n        return false\n    end\n\n    adtype = Optimization.AutoZygote()\n    optf = OptimizationFunction((p, _ , u, y_n, y_b) -> loss(p, u, y_n, y_b), adtype)\n    optproblem = OptimizationProblem(optf, p)\n    result = Optimization.solve(optproblem, ADAMW(1e-3), ncycle(train_loader, epochs); callback)\n    return model, θ_best\n    \nend\n","category":"page"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"model, θ_best = train(model, p, st, train_loader, val_loader, 5000, 500);","category":"page"},{"location":"examples/Joint_area2bump/","page":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","title":"Infering neural and behavioral dynamics in Area2 during perturbed reach task","text":"u, y_n, y_b = first(test_loader) \n(ŷ_n, ŷ_b), _, x = predict(model, y_n, u, ts, θ_best, st, 20)\nsample = 2\nch = 15\nŷₘ = dropmean(ŷ_n, dims=4)\nŷₛ = dropmean(ŷ_n, dims=4)\ndist = Poisson.(ŷₘ)\npred_spike = rand.(dist)\nxₘ = dropmean(x, dims=4)\nval_bps = bits_per_spike(ŷₘ, y_n)\n\np1 = plot(transpose(y_n[ch:ch,:,sample]), label=\"True Spike\", lw=2)\np2 = plot(transpose(pred_spike[ch:ch,:,sample]), label=\"Predicted Spike\", lw=2, color=\"red\")\np3 = plot(transpose(ŷₘ[ch:ch,:,sample]), ribbon=transpose(ŷₛ[ch:ch,:,sample]), label=\"Infered rates\", lw=2, color=\"green\")\n@printf(\"Validation bits/spike: %.2f\\n\", val_bps)\n\nplot(p1, p2,p3, layout=(3,1), size=(800, 400), legend=:topleft)\n","category":"page"},{"location":"tutorials/1-setting_up_model/#Building-a-differentiable-model","page":"Setting up a differentiable model","title":"Building a differentiable model","text":"","category":"section"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"In this tutorial, we will set up a differentiable FitzHugh-Nagumo model using the NeuroDynamics package and plot the phase portrait of the model before training.","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"using NeuroDynamics, Lux, Random, Plots, DifferentialEquations, ComponentArrays, Base.Iterators, CairoMakie","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"We will create the model initialized using the default parameters. To change the parameters, we can pass the desired values to the FitzHughNagumo constructor.","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"rng = MersenneTwister(3)\nvector_field = FitzHughNagumo()\nmodel = ODE(vector_field, Tsit5())\np, st = Lux.setup(rng, model) \np = p |> ComponentArray","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"Setup the initial conditions, inputs and the time span for the simulation. We will set up the input to nothing for now (an unforced system). We will then plot the solution. ","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"ts = 0.0:0.1:10.0\nx0 = rand(2, 1)\nu = nothing\nsol = model(x0, u, ts, p, st)[1]\nplot(sol, idx=[1,2],  xlabel=\"Time\", title=\"FitzHugh-Nagumo\", label=[\"V\" \"W\"], linewidth=2)","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"(Image: FitzHugh-Nagumo)","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"We can also plot the phase portrait of the model using the phaseplot function.","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"v_ranges = -3.0:0.1:3.0\nw_ranges = -3.0:0.1:3.0\nx₀_ranges = collect(product(v_ranges, w_ranges))\nphaseplot(model, x₀_ranges, u, ts, p, st)","category":"page"},{"location":"tutorials/1-setting_up_model/","page":"Setting up a differentiable model","title":"Setting up a differentiable model","text":"(Image: FitzHugh-Nagumo)","category":"page"},{"location":"examples/Modeling_HodkingHuxely/#Modeling-Hodking-Huxely-with-latent-neural-ODEs","page":"Modeling single neuron","title":"Modeling Hodking-Huxely with latent neural ODEs","text":"","category":"section"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"In this example will show how to use the latentUDE framework to model a Hodking-Huxely neuron with dynamic synaptic inputs. ","category":"page"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"using Pkg, Revise, Lux, Random, DifferentialEquations, SciMLSensitivity, ComponentArrays, Plots, MLUtils, OptimizationOptimisers, LinearAlgebra, Statistics, Printf\nusing IterTools: ncycle\nusing NeuroDynamics","category":"page"},{"location":"examples/Modeling_HodkingHuxely/#1.Generating-ground-truth-data","page":"Modeling single neuron","title":"1.Generating ground truth data","text":"","category":"section"},{"location":"examples/Modeling_HodkingHuxely/#1.1-Simulating-Synaptic-Inputs","page":"Modeling single neuron","title":"1.1 Simulating Synaptic Inputs","text":"","category":"section"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"We will use the Tsodyks-Markram model to simulate the synaptic inputs to a neuron. We will generate multiple trajectories to later drive our Hodking-Huxley neuron model.","category":"page"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"n_samples = 64\ntspan = (0.0, 500.0)\nts = range(tspan[1], tspan[2], length=100)\np =  [30, 1000, 50, 0.5, 0.005]\nfunction TMS(x, p, t)\n    v, R, gsyn = x\n    tau, tau_u, tau_R, v0, gmax = p \n    dx₁ = -(v / tau_u)\n    dx₂ = (1 - R) / tau_R\n    dx₃ = -(gsyn / tau)\n    return vcat(dx₁, dx₂, dx₃)\nend\n\nfunction epsp!(integrator)\n    integrator.u[1] += integrator.p[4] * (1 - integrator.u[1])\n    integrator.u[3] += integrator.p[5] * integrator.u[1] * integrator.u[2]\n    integrator.u[2] -= integrator.u[1] * integrator.u[2]\nend\nprob = ODEProblem(TMS, [0.0, 1.0, 0.0], tspan, p)\nfunction prob_func(prob, i, repeat)\n    t_start = rand(50:100)\n    t_int = rand(50:100)\n    t_end = rand(400:450)\n    epsp_ts = PresetTimeCallback(t_start:t_int:t_end, epsp!, save_positions=(false, false))\n    remake(prob, callback=epsp_ts)\nend\n\nensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\nU = solve(ensemble_prob, Tsit5(),  EnsembleThreads(); saveat=ts, trajectories=n_samples);\nplot(U, vars=(1), alpha=0.5, color=:blue, lw=0.5, legend=false, xlabel=\"Time (ms)\", ylabel=\"Membrane Potential (mV)\")\n","category":"page"},{"location":"examples/Modeling_HodkingHuxely/#1.2-Simulating-a-Hodgkin-Huxley-Neuron","page":"Modeling single neuron","title":"1.2 Simulating a Hodgkin-Huxley Neuron","text":"","category":"section"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"# Potassium ion-channel rate functions\nalpha_n(v) = (0.02 * (v - 25.0)) / (1.0 - exp((-1.0 * (v - 25.0)) / 9.0))\nbeta_n(v) = (-0.002 * (v - 25.0)) / (1.0 - exp((v - 25.0) / 9.0))\n\n# Sodium ion-channel rate functions\nalpha_m(v) = (0.182 * (v + 35.0)) / (1.0 - exp((-1.0 * (v + 35.0)) / 9.0))\nbeta_m(v) = (-0.124 * (v + 35.0)) / (1.0 - exp((v + 35.0) / 9.0))\n\nalpha_h(v) = 0.25 * exp((-1.0 * (v + 90.0)) / 12.0)\nbeta_h(v) = (0.25 * exp((v + 62.0) / 6.0)) / exp((v + 90.0) / 12.0)\n\n\n\nfunction HH(x, p, t, u)\n    gK, gNa, gL, EK, ENa, EL, C, ESyn, i = p\n    v, n, m, h = x\n    ISyn(t) = u[i](t)[end] * (ESyn - v)\n\n    dx₁ = ((gK * (n^4.0) * (EK - v)) + (gNa * (m^3.0) * h * (ENa - v)) + (gL * (EL - v)) + ISyn(t)) / C\n    dx₂ = (alpha_n(v) * (1.0 - n)) - (beta_n(v) * n)\n    dx₃ = (alpha_m(v) * (1.0 - m)) - (beta_m(v) * m)\n    dx₄ = (alpha_h(v) * (1.0 - h)) - (beta_h(v) * h)\n\n    dx = vcat(dx₁, dx₂, dx₃, dx₄)\nend\n\ndxdt(x, p, t) = HH(x, p, t, U)\n\np = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0, 1] \n# n, m & h steady-states\nn_inf(v) = alpha_n(v) / (alpha_n(v) + beta_n(v))\nm_inf(v) = alpha_m(v) / (alpha_m(v) + beta_m(v))\nh_inf(v) = alpha_h(v) / (alpha_h(v) + beta_h(v))\n\nv0 = -60\nx0 = [v0, n_inf(v0), m_inf(v0), h_inf(v0)]\nprob = ODEProblem(dxdt, x0, tspan, p)\nprob_func(prob, i, repeat) = remake(prob, p=(p[1:end-1]..., i))\nensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\nY = solve(ensemble_prob, EnsembleThreads(); saveat=ts, trajectories=n_samples)\nplot(Y, vars=1, label=\"v\")\n","category":"page"},{"location":"examples/Modeling_HodkingHuxely/#1.3-Creating-a-dataset-and-splitting-it-into-train-val-test-sets","page":"Modeling single neuron","title":"1.3 Creating a dataset and splitting it into train val test sets","text":"","category":"section"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"Y_data = Array(Y) .|> Float32\nU_data = Array(U) .|> Float32\ninput_dim = size(U_data)[1]\nobs_dim = size(Y_data)[1]\n(u_train, y_train), (u_val, y_val) = splitobs((U_data, Y_data); at=0.8, shuffle=true)\n# Create dataloaders\ntrain_loader = DataLoader((U_data, Y_data), batchsize=32, shuffle=false)\nval_loader = DataLoader((U_data, Y_data), batchsize=32, shuffle=true);\n","category":"page"},{"location":"examples/Modeling_HodkingHuxely/#2.-Creating-the-model","page":"Modeling single neuron","title":"2. Creating the model","text":"","category":"section"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"function create_model(n_states, ctrl_dim, obs_dim, context_dim, t_init)\n    rng = Random.MersenneTwister(1234)\n    obs_encoder = Recurrent_Encoder(obs_dim, n_states, context_dim, 32, t_init)\n    vector_field = Chain(Dense(n_states+ctrl_dim, 32, softplus), Dense(32, n_states, tanh))\n    dynamics = ODE(vector_field, Euler(); saveat=ts, dt=2.0)\n    obs_decoder = Linear_Decoder(n_states, obs_dim, \"None\")   \n\n    model = LatentUDE(obs_encoder=obs_encoder, dynamics=dynamics, obs_decoder=obs_decoder)\n    p, st = Lux.setup(rng, model)\n    p = p |> ComponentArray{Float32}\n    return model, p, st\nend","category":"page"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"latent_dim = 8\ncontext_dim = 0 # No need for context if we have ODE dynamics\nt_init = 50\nmodel, p, st = create_model(latent_dim, input_dim, obs_dim, context_dim, t_init)\nu, y = first(train_loader)\nts = ts |> Array{Float32};","category":"page"},{"location":"examples/Modeling_HodkingHuxely/#3.-Train-the-model-via-variational-inference","page":"Modeling single neuron","title":"3. Train the model via variational inference","text":"","category":"section"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"function train(model, p, st, train_loader, val_loader, epochs, print_every)\n    \n    epoch = 0\n    L = frange_cycle_linear(epochs+1, 0.0f0, 1.0f0, 1, 0.5)\n    losses = []\n    best_model_params = nothing\n    best_metric = Inf\n    function loss(p, u, y, ts=ts)\n        ŷ, û, x̂₀, _ = model(y, u, ts, p, st)\n        batch_size = size(y)[end]\n        recon_loss = mse(ŷ[1:1, :, :], y[1:1, :, :])/batch_size\n        kl_loss = kl_normal(x̂₀[1], x̂₀[2])/batch_size\n        l =  0.1*recon_loss + L[epoch+1]*kl_loss\n        return l, recon_loss, kl_loss\n    end\n\n\n    callback = function(opt_state, l, recon_loss, kl_loss)\n        θ = opt_state.u\n        push!(losses, l)\n        if length(losses) % length(train_loader) == 0\n            epoch += 1\n        end\n\n        if length(losses) % (length(train_loader)*print_every) == 0\n            @printf(\"Current epoch: %d, Loss: %.2f, Reconstruction: %d, KL: %.2f\\n\", epoch, losses[end], recon_loss, kl_loss)\n            u, y = first(val_loader)\n            batch_size = size(y)[end]\n            ŷ, _, x = predict(model, y, u, ts, θ, st, 20)\n            ŷ_mean = dropdims(mean(ŷ, dims=4), dims=4)\n            val_mse = mse(ŷ_mean[1:1, :, :], y[1:1, :, :])\n            @printf(\"Validation MSE: %.2f\\n\", val_mse)\n            if val_mse < best_metric\n                best_metric = val_mse\n                @printf(\"Saving model with best metric: %.2f\\n\", best_metric)\n                best_model_params = copy(θ)\n\n            end\n\n            pl = plot(transpose(y[1:1, :, 1]), label=\"True\", lw=2.0)\n            plot!(pl, transpose(ŷ_mean[1:1, :, 1]), label=\"Predicted\", lw=2.0, xlabel=\"Time (ms)\", ylabel=\"Membrane Potential (mV)\")\n            display(pl)\n        \n        end\n        return false\n    end\n\n    adtype = Optimization.AutoZygote()\n    optf = OptimizationFunction((p, _ , u, y) -> loss(p, u, y), adtype)\n    optproblem = OptimizationProblem(optf, p)\n    result = Optimization.solve(optproblem, ADAMW(1e-3), ncycle(train_loader, epochs); callback)\n    return result, losses, model, best_model_params\n    \nend\n","category":"page"},{"location":"examples/Modeling_HodkingHuxely/","page":"Modeling single neuron","title":"Modeling single neuron","text":"result, losses, model, best_p = train(model, p, st, train_loader, val_loader, 5000, 50)\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = NeuroDynamics","category":"page"},{"location":"#NeuroDynamics.jl:-Generative-Modeling-of-Neural-Dynamics","page":"Home","title":"NeuroDynamics.jl: Generative Modeling of Neural Dynamics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"NeuroDynamics.jl is a Julia package for scalable and efficient generative modeling of neural dynamics. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"It provides the necessary tools to infer and simulate the dynamics of neural systems from empirical data (e.g., electrophysiological recordings, calcium imaging data, behavioural recordings, etc.). The whole idea is viewing these empirical data as partial noisy observations of a underlying continous stochastic process which we model as a system of stochastic differential equations (SDE) that we try to infer.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Dynamical systems perspective)","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nNeuroDynamics.jl is still in its enfancy and under active development.  We welcome contributions from the community to help improve the package.","category":"page"},{"location":"#Feature-Summary","page":"Home","title":"Feature Summary","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The current key features of NeuroDynamics.jl include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A library of models of neural dynamics across different scales of organization and abastraction (e.g., single neurons, neural populations, networks of neural populations).\nA flexible and user-friendly interface for specifying differntiable and composable neural models.\nA scalable and efficient inference approach for infering the parameters of (Universal) ODEs/SDEs. \nSampling, simulation, and visulization of the dynamics of infered systems. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install NeuroDynamics.jl, you can run the following command in the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.dev(\"https://github.com/elgazzarr/NeuroDynamics.jl\")","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To get started with NeuroDynamics.jl, we recommend checking out the tutorials and examples.","category":"page"},{"location":"#Citing-NeuroDynamics.jl","page":"Home","title":"Citing NeuroDynamics.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use NeuroDynamics.jl in your research, please consider citing the following paper:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{elgazzar2024universal,\n  title={Universal Differential Equations as a Common Modeling Language for Neuroscience},\n  author={ElGazzar, Ahmed and van Gerven, Marcel},\n  journal={arXiv preprint arXiv:2403.14510},\n  year={2024}\n}","category":"page"},{"location":"refs/","page":"API","title":"API","text":"","category":"page"},{"location":"refs/","page":"API","title":"API","text":"Modules = [NeuroDynamics]","category":"page"},{"location":"refs/#NeuroDynamics.Decoder","page":"API","title":"NeuroDynamics.Decoder","text":"Decoder\n\nA decoder is a function that takes a latent variable and produces an output (Observations or Control inputs).    \n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.Decoder-Tuple{Any, Any, Any}","page":"API","title":"NeuroDynamics.Decoder","text":"(model::Decoder)(x::AbstractArray, p::ComponentVector, st::NamedTuple)\n\nThe forward pass of the decoder.\n\nArguments:\n\nx: The input to the decoder.\np: The parameters.\nst: The state.\n\nreturns:\n\n- 'ŷ': The output of the decoder.\n- 'st': The state of the decoder.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.Encoder","page":"API","title":"NeuroDynamics.Encoder","text":"Encoder\n\nAn encoder is a container layer that contains three sub-layers: linear_net, init_net, and context_net.\n\nFields\n\nlinear_net: A layer that maps the input to a hidden representation.\ninit_net: A layer that maps the hidden representation to the initial hidden state.\ncontext_net: A layer that maps the hidden representation to the context.\n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.Encoder-Tuple{Any, Any, Any}","page":"API","title":"NeuroDynamics.Encoder","text":"(model::Encoder)(x::AbstractArray, p::ComponentVector, st::NamedTuple)\n\nThe forward pass of the encoder.\n\nArguments:\n\nx: The input to the encoder (e.g. observations).\np: The parameters.\nst: The state of the encoder.\n\nreturns:\n\n- `x̂₀`: The initial hidden state.\n- `context`: The context.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.HarmonicOscillators","page":"API","title":"NeuroDynamics.HarmonicOscillators","text":"HarmonicOscillators\n\nA struct that defines a system of harmonic oscillators.\n\nArguments\n\n`N' : Number of oscillators\n`M' : Number of inputs\n`ω' : Frequency parameter ω\n`γ' : Damping coefficient γ\n`K' : Coupling matrix K\n`B' : External input coupling matrix B\n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.HarmonicOscillators-Tuple{Any, AbstractArray, Any, Any, Any}","page":"API","title":"NeuroDynamics.HarmonicOscillators","text":"(m::HarmonicOscillators)(x, u, t, p, st)\n\nThe forward pass of the HarmonicOscillators.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.LatentUDE","page":"API","title":"NeuroDynamics.LatentUDE","text":"LatentUDE(obs_encoder, ctrl_encoder, dynamics, obs_decoder, ctrl_decoder)\n\nConstructs a Latent Universal Differential Equation model.\n\nArguments:\n\nobs_encoder: A function that encodes the observations y to get the initial hidden state x₀ and context for the dynamics if needed (Partial observability) \nctrl_encoder: A function that encodes (high-dimensional) inputs/controls to a lower-dimensional representation if needed.\ndynamics: A function that models the dynamics of the system (your ODE/SDE).\nobs_decoder: A function that decodes the hidden states x to the observations y.\n'ctrl_decoder': A function that decodes the control representation to the original control space if needed.\n'device': The device on which the model is stored. Default is cpu. \n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.LatentUDE-Tuple{AbstractArray, Union{Nothing, AbstractArray}, AbstractArray, ComponentArrays.ComponentArray, NamedTuple}","page":"API","title":"NeuroDynamics.LatentUDE","text":"(model::LatentUDE)(y::AbstractArray, u::Union{Nothing, AbstractArray}, ts::AbstractArray, ps::ComponentArray, st::NamedTuple)\n\nThe forward pass of the LatentUDE model.\n\nArguments:\n\ny: Observations\nu: Control inputs\nts: Time points\nps: Parameters\nst: NamedTuple of states \n\nReturns:\n\nŷ: Decoded observations from the hidden states.\nū: Decoded control inputs from the hidden states.\nx̂₀: Encoded initial hidden state.\nkl_path: KL divergence path. (Only for SDE dynamics, otherwise nothing)\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.ModernWilsonCowan","page":"API","title":"NeuroDynamics.ModernWilsonCowan","text":"Modern Wilson-Cowan Model of Neural Populations \n\nThe modern Wilson-Cowan model is a system of ordinary differential equations that describes the dynamics of excitatory and inhibitory neural populations with external inputs.\n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.ODE","page":"API","title":"NeuroDynamics.ODE","text":"ODE(vector_field, solver; kwargs...)\n\nConstructs an ODE model.\n\nArguments:\n\nvector_field: The vector field of the ODE. \n`solver': The nummerical solver used to solve the ODE.\nkwargs: Additional keyword arguments to pass to the solver.\n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.ODE-Tuple{AbstractArray, Union{Nothing, AbstractArray}, AbstractArray, ComponentArrays.ComponentArray, NamedTuple}","page":"API","title":"NeuroDynamics.ODE","text":"(de::ODE)(x::AbstractArray, u::Union{Nothing, AbstractArray}, ts::AbstractArray, p::ComponentVector, st::NamedTuple)\n\nThe forward pass of the ODE.\n\nArguments:\n\nx: The initial hidden state.\nu: The control input.\nts: The time steps.\np: The parameters.\nst: The state.\n\nreturns:      - The solution of the ODE.     - The state of the model.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.SDE","page":"API","title":"NeuroDynamics.SDE","text":"SDE(drift, drift_aug, diffusion, solver; kwargs...)\n\nConstructs an SDE model.\n\nArguments:\n\ndrift: The drift of the SDE. \ndrift_aug: The augmented drift of the SDE. Used only for training.\ndiffusion: The diffusion of the SDE.\n`solver': The nummerical solver used to solve the SDE.\nkwargs: Additional keyword arguments to pass to the solver.\n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.SDE-Tuple{AbstractArray, Union{Nothing, AbstractArray}, AbstractArray, AbstractArray, ComponentArrays.ComponentVector, NamedTuple}","page":"API","title":"NeuroDynamics.SDE","text":"(de::SDE)(x::AbstractArray, u::AbstractArray, c::AbstractArray, ts::StepRangeLen, p::ComponentVector, st::NamedTuple)\n\nThe forward pass of the SDE.\n\nArguments:\n\nx: The initial hidden state.\nu: The control input.\nc: The context.\nts: The time steps.\np: The parameters.\nst: The state.\n\nreturns:      - The solution of the SDE.     - The state of the model.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.WilsonCowan","page":"API","title":"NeuroDynamics.WilsonCowan","text":"Wilson-Cowan Model of Neural Populations \n\nThe classic Wilson-Cowan model is a system of ordinary differential equations that describes the dynamics of excitatory and inhibitory neural populations.\n\n\n\n\n\n","category":"type"},{"location":"refs/#NeuroDynamics.Identity_Decoder-Tuple{}","page":"API","title":"NeuroDynamics.Identity_Decoder","text":"Identity_Decoder()\n\nConstructs an identity decoder. Useful for fully observable systems.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.Identity_Encoder-Tuple{}","page":"API","title":"NeuroDynamics.Identity_Encoder","text":"Identity_Encoder()\n\nConstructs an identity encoder. Useful for fully observable systems.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.Linear_Decoder","page":"API","title":"NeuroDynamics.Linear_Decoder","text":"Linear_Decoder(obs_dim, latent_dim)\n\nConstructs a linear decoder.\n\nArguments:\n\nobs_dim: Dimension of the observations.\nlatent_dim: Dimension of the latent space. \nnoise: Type of observation noise. Default is Gaussian. Options are Gaussian, Poisson, None.\n\nreturns: \n\n- The decoder.\n\n\n\n\n\n","category":"function"},{"location":"refs/#NeuroDynamics.MLP_Decoder","page":"API","title":"NeuroDynamics.MLP_Decoder","text":"MLP_Decoder(obs_dim, latent_dim, hidden_dim, n_hidden)\n\nConstructs an MLP decoder.\n\nArguments:\n\nobs_dim: Dimension of the observations.\nlatent_dim: Dimension of the latent space.\nhidden_dim: Dimension of the hidden layers.\nn_hidden: Number of hidden layers.\nnoise: Type of observation noise. Default is Gaussian. Options are Gaussian, Poisson, None.\n\nreturns: \n\n- The decoder.\n\n\n\n\n\n","category":"function"},{"location":"refs/#NeuroDynamics.Nothing_Decoder-Tuple{}","page":"API","title":"NeuroDynamics.Nothing_Decoder","text":"Nothing_Decoder()\n\nConstructs a decoder that does nothing. \n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.Recurrent_Encoder-NTuple{5, Any}","page":"API","title":"NeuroDynamics.Recurrent_Encoder","text":"Recurrent_Encoder(obs_dim, latent_dim, context_dim, hidden_dim, t_init)\n\nConstructs a recurrent encoder.\n\nArguments:\n\nobs_dim: Dimension of the observations.\nlatent_dim: Dimension of the latent space.\ncontext_dim: Dimension of the context.\nhidden_dim: Dimension of the hidden state.\nt_init: Number of initial time steps to use for the initial hidden state.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.bits_per_spike-Tuple{Any, Any}","page":"API","title":"NeuroDynamics.bits_per_spike","text":"bits_per_spike(rates, spikes)\n\nCompute the bits per spike by comparing the Poisson log-likelihood of the rates with the Poisson log-likelihood of the mean spikes. \n\nArguments:\n\nrates: The predicted rates.\nspikes: The observed spikes.\n\nreturns: \n\n- The bits per spike.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.forward!-Tuple{LatentUDE, AbstractArray, Union{Nothing, AbstractArray}, AbstractArray, ComponentArrays.ComponentArray, NamedTuple, ODE}","page":"API","title":"NeuroDynamics.forward!","text":"forward!(model::LatentUDE, y::AbstractArray, u::Union{Nothing, AbstractArray}, ts::AbstractArray, ps::ComponentArray, st::NamedTuple, dynamics::ODE)\n\nThe forward pass of the LatentODE model.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.forward!-Tuple{LatentUDE, AbstractArray, Union{Nothing, AbstractArray}, AbstractArray, ComponentArrays.ComponentArray, NamedTuple, SDE}","page":"API","title":"NeuroDynamics.forward!","text":"forward!(model::LatentUDE, y::AbstractArray, u::Union{Nothing, AbstractArray}, ts::AbstractArray, ps::ComponentArray, st::NamedTuple, dynamics::SDE)\n\nThe forward pass of the LatentSDE model.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.frange_cycle_linear-Union{Tuple{Any}, Tuple{T}, Tuple{Any, T}, Tuple{Any, T, T}, Tuple{Any, T, T, Any}, Tuple{Any, T, T, Any, Any}} where T","page":"API","title":"NeuroDynamics.frange_cycle_linear","text":"frange_cycle_linear(n_iter, start, stop, n_cycle, ratio)\n\nGenerate a linear schedule with cycles.\n\nArguments:\n\nn_iter: Number of iterations.\nstart: Start value.\nstop: Stop value.\nn_cycle: Number of cycles.\nratio: Ratio of the linear schedule.\n\nreturns: \n\n- The linear schedule.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.interp!-Tuple{Any, AbstractArray, Any}","page":"API","title":"NeuroDynamics.interp!","text":"interp!(ts, cs, time_point)\n\nInterpolates the control signal at a given time point.\n\nArguments:\n\nts: Array of time points.\ncs: Array of control signals.\ntime_point: The time point at which to interpolate the control signal.\n\nreturns: \n\n- The interpolated control signal.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.kl_normal-Tuple{Any, Any}","page":"API","title":"NeuroDynamics.kl_normal","text":"kl_normal(μ, σ²)\n\nCompute the KL divergence between a normal distribution and a standard normal distribution.\n\nArguments:\n\nμ: Mean of the normal distribution.\nσ²: Variance of the normal distribution.\n\nreturns: \n\n- The KL divergence.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.mse-Tuple{Any, Any}","page":"API","title":"NeuroDynamics.mse","text":"mse(ŷ, y)\n\nCompute the mean squared error.\n\nArguments:\n\nŷ: Predicted values.\ny: Observed values.\n\nreturns: \n\n- The mean squared error.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.normal_loglikelihood-Tuple{Any, Any, Any}","page":"API","title":"NeuroDynamics.normal_loglikelihood","text":"normal_loglikelihood(μ, σ², y)\n\nCompute the log-likelihood of a normal distribution.\n\nArguments:\n\nμ: Mean of the normal distribution.\nσ²: Variance of the normal distribution.\ny: The observed values.\n\nreturns: \n\n- The log-likelihood.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.phaseplot-Tuple{ODE, Vararg{Any, 5}}","page":"API","title":"NeuroDynamics.phaseplot","text":"phaseplot(de::ODE, x₀_ranges, u, ts, p, st; kwargs...)\n\nPlots the phase portrait of the ODE model.\n\nArguments:\n\nde: The ODE model.\nx₀_ranges: The initial condition ranges.\nu: The control input.\nts: The time steps.\np: The parameters.\nst: The state of the model.\nkwargs: Additional keyword arguments for plotting.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.poisson_loglikelihood-Tuple{Any, Any}","page":"API","title":"NeuroDynamics.poisson_loglikelihood","text":"poisson_loglikelihood(λ, y)\n\nCompute the log-likelihood of a Poisson distribution.\n\nArguments:\n\nλ: The rate of the Poisson distribution.\ny: The observed spikes.\n\nreturns: \n\n- The log-likelihood.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.predict-Tuple{LatentUDE, AbstractArray, Union{Nothing, AbstractArray}, AbstractArray, ComponentArrays.ComponentArray, NamedTuple, Int64}","page":"API","title":"NeuroDynamics.predict","text":"predict(model::LatentUDE, y::AbstractArray, u::Union{Nothing, AbstractArray}, ts::AbstractArray, ps::ComponentArray, st::NamedTuple, n_samples::Int)\n\nSamples trajectories from the LatentUDE model.\n\nArguments:\n\nmodel: The LatentUDE model to sample from.\ny: Observations used to encode the initial hidden state. \nu: Inputs for the input encoder. Can be Nothing or an array.\nts: Array of time points at which to sample the trajectories.\nps: Parameters for the model.\nst: NamedTuple of states for different components of the model.\nn_samples: Number of samples used to make the prediction.\n\nReturns:\n\nŷ: Decoded observations from the sampled hidden states * n_samples.\nū: Decoded control inputs from the sampled hidden states * n_samples.\nx: Sampled hidden state trajectories * n_samples.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.sample_dynamics-Tuple{ODE, Vararg{Any, 6}}","page":"API","title":"NeuroDynamics.sample_dynamics","text":"sample_dynamics(de::ODE, x̂₀, u, ts, p, st, n_samples)\n\nSamples trajectories from the ODE model.\n\nArguments:\n\nde: The ODE model to sample from.\nx̂₀: The initial hidden state.\nu: Inputs for the input encoder. Can be Nothing or an array.\nts: Array of time points at which to sample the trajectories.\np: The parameters.\nst: The state.\nn_samples: The number of samples to generate.\n\nreturns:      - The sampled trajectories.     - The state of the model.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.sample_dynamics-Tuple{SDE, Vararg{Any, 6}}","page":"API","title":"NeuroDynamics.sample_dynamics","text":"sample_dynamics(de::SDE, x̂₀, u, ts, p, st, n_samples)\n\nSamples trajectories from the SDE model.\n\nArguments:\n\nde: The SDE model to sample from.\nx̂₀: The initial hidden state.\nu: Inputs for the input encoder. Can be Nothing or an array.\nts: Array of time points at which to sample the trajectories.\np: The parameters.\nst: The state.\nn_samples: The number of samples to generate.\n\nreturns:      - The sampled trajectories.     - The state of the model.\n\n\n\n\n\n","category":"method"},{"location":"refs/#NeuroDynamics.sample_rp-Tuple{Tuple}","page":"API","title":"NeuroDynamics.sample_rp","text":"sample_rp(x::Tuple)\n\nSamples from a MultiVariate Normal distribution using the reparameterization trick.\n\nArguments:\n\nx: Tuple of the mean and squared variance of a MultiVariate Normal distribution.\n\nreturns: \n\n- The sampled value.\n\n\n\n\n\n","category":"method"},{"location":"examples/Joint_mcmaze/#Joint-modeling-of-neural-and-behavioural-dynamics-during-dealyed-reach-task","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Joint modeling of neural and behavioural dynamics during dealyed reach task","text":"","category":"section"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"In this example, we will show how to use the latentsde model to generate neural observations (spiking recordings of neurons in the dorsal premotor (PMd) and primary motor (M1) cortices) and behavioural observations (Hand velocity) of a monkey doing a dealyed reach task.  The data is available for download here.","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"using Pkg, Revise, Lux, LuxCUDA, CUDA, Random, DifferentialEquations, SciMLSensitivity, ComponentArrays, Plots, MLUtils, OptimizationOptimisers, LinearAlgebra, Statistics, Printf, PyCall, Distributions\nusing IterTools: ncycle\nusing NeuroDynamics\nnp = pyimport(\"numpy\")\ndevice = \"cpu\"\nconst dev = device == \"gpu\" ? gpu_device() : cpu_device()\n","category":"page"},{"location":"examples/Joint_mcmaze/#1.-Loading-the-data-and-creating-the-dataloaders","page":"Infering neural and behavioral dynamics in delayed reach task","title":"1. Loading the data and creating the dataloaders","text":"","category":"section"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"You can prepare the data yourself or use our preprocessed data staright away which is available here","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"file_path = \"/Users/ahmed.elgazzar/Datasets/NLB/mc_maze.npy\" # Replace with your path to the dataset\ndata = np.load(file_path, allow_pickle=true)\nY_neural = permutedims(get(data[1], \"spikes\") , [3, 2, 1])|> Array{Float32}\nY_behaviour = permutedims(get(data[1], \"hand_vel\") , [3, 2, 1])|> Array{Float32}\nn_neurons = size(Y_neural)[1]\nn_neurons , n_timepoints, n_trials = size(Y_neural);\nn_behviour = size(Y_behaviour)[1]\nts = range(0, 4, length=n_timepoints);\nts_input = repeat(ts, 1, n_trials) \nU = reshape(ts_input, (1, size(ts_input)...))|> Array{Float32} \nn_ctrl = size(U)[1]\n(U_train, Yn_train, Yb_train) , (U_test, Yn_test, Yb_test) = splitobs((U, Y_neural, Y_behaviour); at=0.7)\ntrain_loader = DataLoader((U_train, Yn_train, Yb_train), batchsize=28, shuffle=true)\nval_loader = DataLoader((U_test, Yn_test, Yb_test), batchsize=10, shuffle=true);","category":"page"},{"location":"examples/Joint_mcmaze/#2.-Defining-the-model","page":"Infering neural and behavioral dynamics in delayed reach task","title":"2. Defining the model","text":"","category":"section"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"We will use a \"Recurrent_Encoder\" to infer the initial hidden state from a portion of the observations. \nWe will use a BlackBox (Neural) SDE with multiplicative noise to model the latent dynamics.\nWe will use a multi-headed decoder, one for the neural observations and one for behaviour.","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"hp = Dict(\"n_states\" => 16, \"hidden_dim\" => 64, \"context_dim\" => 32, \"t_init\" => Int(0.8 * n_timepoints))\nrng = Random.MersenneTwister(1234)\nobs_encoder = Recurrent_Encoder(n_neurons, hp[\"n_states\"], hp[\"context_dim\"], 32, hp[\"t_init\"])\ndrift = Chain(Dense(hp[\"n_states\"], hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"], tanh))\ndrift_aug = Chain(Dense(hp[\"n_states\"] + hp[\"context_dim\"] + n_ctrl, hp[\"hidden_dim\"], softplus), Dense(hp[\"hidden_dim\"], hp[\"n_states\"],tanh))\ndiffusion = Scale(hp[\"n_states\"], sigmoid, init_weight=identity_init(gain=0.1))\ndynamics = SDE(drift, drift_aug, diffusion, EulerHeun(), saveat=ts, dt=ts[2]-ts[1])\nobs_decoder = Chain(MLP_Decoder(hp[\"n_states\"], n_neurons, 64, 1, \"Poisson\"), Lux.BranchLayer(NoOpLayer(), Linear_Decoder(n_neurons, n_behviour,\"Gaussian\")))\n\nctrl_encoder, ctrl_decoder = NoOpLayer(), NoOpLayer()\nmodel = LatentUDE(obs_encoder, ctrl_encoder, dynamics, obs_decoder, ctrl_decoder, dev)\np, st = Lux.setup(rng, model)\np = p |> ComponentArray{Float32};\n","category":"page"},{"location":"examples/Joint_mcmaze/#3.-Training-the-model","page":"Infering neural and behavioral dynamics in delayed reach task","title":"3. Training the model","text":"","category":"section"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"We will train the model using the AdamW optimizer with a learning rate of 1e-3 for 200 epochs. ","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"function train(model::LatentUDE, p, st, train_loader, val_loader, epochs, print_every)\n    \n    epoch = 0\n    L = frange_cycle_linear(epochs+1, 0.5f0, 1.0f0, 1, 0.3)\n    losses = []\n    θ_best = nothing\n    best_metric = -Inf\n    println(\"Training ...\")\n\n    function loss(p, u, y_n, y_b)\n        u, y_n, y_b  = u |> dev, y_n |> dev, y_b |> dev\n        (ŷ_n, ŷ_b), _, x̂₀, kl_path = model(y_n, u, ts, p, st)\n        batch_size = size(y_n)[end]\n        neural_loss = - poisson_loglikelihood(ŷ_n, y_n)/batch_size\n        behaviorual_loss = - normal_loglikelihood(ŷ_b..., y_b)\n        obs_loss = neural_loss + behaviorual_loss\n        kl_init = kl_normal(x̂₀[1], x̂₀[2])\n        kl_path = mean(kl_path[end,:])\n        kl_loss =  kl_path  +  kl_init\n        l =  0.1*obs_loss + 10*L[epoch+1]*kl_loss\n        return l, obs_loss, kl_loss\n    end\n\n\n    callback = function(opt_state, l, obs_loss , kl_loss)\n        θ = opt_state.u\n        push!(losses, l)\n        if length(losses) % length(train_loader) == 0\n            epoch += 1\n        end\n\n        if length(losses) % (length(train_loader)*print_every) == 0\n            @printf(\"Current epoch: %d, Loss: %.2f, Observations_loss: %d, KL: %.2f\\n\", epoch, losses[end], obs_loss, kl_loss)\n            u, y_n, y_b = first(train_loader) \n            (ŷ_n, ŷ_b), _, _ = predict(model, y_n, u, ts, θ, st, 20) \n            ŷ_n = dropdims(mean(ŷ_n, dims=4), dims=4)\n            ŷ_b_m, ŷ_b_s = dropdims(mean(ŷ_b[1], dims=4), dims=4), dropdims(mean(ŷ_b[2], dims=4), dims=4)\n            val_bps = bits_per_spike(ŷ_n, y_n)\n            val_ll = normal_loglikelihood(ŷ_b_m, ŷ_b_s, y_b)\n            @printf(\"Validation bits/spike: %.2f\\n\", val_bps)\n            @printf(\"Validation behaviour log-likelihood: %.2f\\n\", val_ll)\n            if val_ll > best_metric\n                best_metric = val_ll\n                 θ_best = copy(θ)\n                 @printf(\"**** Saving best model ****\\n\")\n                end   \n            d = plot_preds(y_b,  ŷ_b[1])\n            display(d)\n\n        end\n        return false\n    end\n\n    adtype = Optimization.AutoZygote()\n    optf = OptimizationFunction((p, _ , u, y_n, y_b) -> loss(p, u, y_n, y_b), adtype)\n    optproblem = OptimizationProblem(optf, p)\n    result = Optimization.solve(optproblem, ADAMW(5e-4), ncycle(train_loader, epochs); callback)\n    return model, θ_best\n    \nend\n","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"model, θ_best = train(model, p, st, train_loader, val_loader, 5000, 500);","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"u, y_n, y_b = first(train_loader) \n(ŷ_n, ŷ_b), _, x = predict(model, y_n, u, ts, θ_best, st, 20)\nsample = 8\nch = 9\nŷₘ = dropmean(ŷ_n, dims=4)\nŷₛ = dropmean(ŷ_n, dims=4)\ndist = Poisson.(ŷₘ)\npred_spike = rand.(dist)\nxₘ = dropmean(x, dims=4)\nval_bps = bits_per_spike(ŷₘ, y_n)\n\np1 = plot(transpose(y_n[ch:ch,:,sample]), label=\"True Spike\", lw=2)\np2 = plot(transpose(pred_spike[ch:ch,:,sample]), label=\"Predicted Spike\", lw=2, color=\"red\")\np3 = plot(transpose(ŷₘ[ch:ch,:,sample]), ribbon=transpose(ŷₛ[ch:ch,:,sample]), label=\"Infered rates\", lw=2, color=\"green\")\n\nplot(p1, p2,p3, layout=(3,1), size=(800, 400), legend=:topleft)\n","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"savefig(\"spike_prediction.png\")","category":"page"},{"location":"examples/Joint_mcmaze/","page":"Infering neural and behavioral dynamics in delayed reach task","title":"Infering neural and behavioral dynamics in delayed reach task","text":"s = 13\nplot_samples(ŷ_b[1], s)\nplot!(transpose(y_b[:,:,s]), label=[\"True\" nothing], lw=2, color=\"red\", legend=:topleft)\n","category":"page"}]
}
